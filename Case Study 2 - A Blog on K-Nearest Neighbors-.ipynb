{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important topics of the Blog:\n",
    "\n",
    "Part-1: Introduction k-Nearest-Neighbors\n",
    "\n",
    "Part-2: Feature Vectorization Techniques\n",
    "\n",
    "Part-3: The k-Nearest-Neighbors steps to solve a Problem\n",
    "\n",
    "Part-4: Distance Measurement\n",
    "\n",
    "Part-5: The hyper paramter tuning(finding the best K)\n",
    "\n",
    "Part-6: Techniques of choosing neighbors\n",
    "\n",
    "Part-7: Hyperparameter Tuning Techniques\n",
    "\n",
    "Part-8: Performance Measurement of a Classification Model\n",
    "\n",
    "Part-9: Advantages and Disadvantages of KNN\n",
    "\n",
    "Part-10: Applications of KNN\n",
    "\n",
    "Part-11:  Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "\n",
    "1. https://towardsdatascience.com/knn-k-nearest-neighbors-1-a4707b24bd1d\n",
    "    \n",
    "2. https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "3. http://pointclouds.org/documentation/tutorials/kdtree_search.php\n",
    "    \n",
    "4. https://en.wikipedia.org/wiki/Random_search\n",
    "    \n",
    "5. https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "    \n",
    "6. https://www.wired.co.uk/article/how-do-netflixs-algorithms-work-machine-learning-helps-to-predict-what-viewers-will-like\n",
    "\n",
    "7. https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-Nearest-Neighbors (kNN) method of classification is one of the simplest methods in machine learning. At its most basic level, it is essentially classification by finding the most similar data points in the training data. Although very simple to understand and implement, this method has seen wide application in many domains, such as in recommendation systems, semantic searching, and anomaly detection.\n",
    "\n",
    "K Nearest Neighbour is a simple algorithm that stores all the available cases and classifies the new data or case based on a similarity measure. It is mostly used to classifies a data point based on how its neighbours are classified.\n",
    "\n",
    "K — Nearest Neighbors is a supervised learning algorithms used in machine learning, it’s a classifier algorithm where the learning is based “how similar” is a data (a vector) from other .\n",
    "\n",
    "The k-nearest neighbors algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems.It is a supervised machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest-Neighbors:      a supervised machine learning algorithm\n",
    "\n",
    "A supervised machine learning algorithm (as opposed to an unsupervised machine learning algorithm) is one that relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data.\n",
    "\n",
    "So Knn is a Supervised machine learning algorithm as target variable is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Vectorization Techniques</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would need to in any machine learning problem, we must first find a way to represent data points as feature vectors. A feature vector is our mathematical representation of data.\n",
    "\n",
    "### Feature Vecorization Techniques are:\n",
    "1. Bag Of Words\n",
    "2. TF-IDF vectorization.\n",
    "3. Avg w2v.\n",
    "4. TF-IDF w2v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bag Of Words\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words.The bag-of-words model has also been used for computer vision.\n",
    "\n",
    "The bag-of-words model is commonly used in methods of document classification where the occurrence of each word is used as a feature for training a classifier.\n",
    "\n",
    "An early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.\n",
    "\n",
    "### EX. \n",
    "\n",
    "John likes to watch movies. Mary likes movies too.\n",
    "\n",
    "BOW-->  \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-gram model\n",
    "\n",
    "Bag-of-word model is an orderless document representation—only the counts of words mattered. For instance, in the above example \"John likes to watch movies. Mary likes movies too\", the bag-of-words representation will not reveal that the verb \"likes\" always follows a person's name in this text. As an alternative, the n-gram model can store this spatial information. Applying to the same example above, a bigram model will parse the text into the following units and store the term frequency of each unit as before. \n",
    "\n",
    "Bi-gram model-->    [\n",
    "    \"John likes\",\n",
    "    \"likes to\",\n",
    "    \"to watch\",\n",
    "    \"watch movies\",\n",
    "    \"Mary likes\",\n",
    "    \"likes movies\",\n",
    "    \"movies too\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF vectorization\n",
    "\n",
    "TF-IDF stands for “Term Frequency — Inverse Document Frequency”. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.\n",
    "\n",
    "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vec\n",
    "\n",
    "Word2Vec is an efficient solution to these problems, which leverages the context of the target words. Essentially, we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word representation.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of KNN are:\n",
    "\n",
    "1 — Receive an unclassified data\n",
    "\n",
    "2 — Compute a distance value between the item to be classified and every item in the training data-set\n",
    "\n",
    "3 — Pick the k closest data points (the items with the k lowest distances)\n",
    "\n",
    "4 — Conduct a “majority vote” among those data points — the dominating classification in that pool is decided as the final classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance Measurement\n",
    "\n",
    "Distance can be calculated using\n",
    "\n",
    "1. Euclidean distance\n",
    "2. Manhattan distance\n",
    "3. Hamming Distance\n",
    "4. Minkowski Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Euclidean distance: \n",
    "\n",
    "  Euclidean distance is the square root of the sum of squared distance between two points. It is also known as L2 norm.\n",
    "\n",
    "#### 2. Manhattan distance:\n",
    "\n",
    "  Manhattan distance is the sum of the absolute values of the differences between two points\n",
    "\n",
    "#### 3. Hamming distance\n",
    "\n",
    "   Hamming distance is used for categorical variables. In simple terms it tells us if the two categorical variables are same or not.\n",
    "\n",
    "#### 4. Minkowski distance\n",
    "\n",
    "   Minkowski distance is the used to find distance similarity between two points. When p=1, it becomes Manhattan distance and when p=2, it becomes Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The hyper paramter tuning(finding the best K)</h2>\n",
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done with training our model, we just can’t assume that it is going to work well on data that it has not seen before. In other words, we can not be sure that the model will have the desired accuracy and variance in production environment. We need some kind of assurance of the accuracy of the predictions that our model is putting out. For this, we need to validate our model. This process of deciding whether the numerical results quantifying hypothesised relationships between variables, are acceptable as descriptions of the data, is known as validation.\n",
    "\n",
    "To evaluate the performance of any machine learning model we need to test it on some unseen data. Based on the models performance on unseen data we can say weather our model is Under-fitting/Over-fitting/Well generalised. Cross validation (CV) is one of the technique used to test the effectiveness of a machine learning models, it is also a re-sampling procedure used to evaluate a model if we have a limited data. To perform CV we need to keep aside a sample/portion of the data on which is do not use to train the model, later us this sample for testing/validating.\n",
    "\n",
    "Below are the common techniques used for CV.\n",
    "\n",
    "### 10-Folds Cross Validation:\n",
    "\n",
    "10-Fold is a popular and easy to understand, it generally results in a less biased model compare to other methods. Because it ensures that every observation from the original dataset has the chance of appearing in training and test set. This is one among the best approach if we have a limited input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Techniques of choosing neighbors?</h2>\n",
    "\n",
    "Techniques of choosing neighbors are:\n",
    "\n",
    "1. Brute Force Algorithm\n",
    "\n",
    "2. K-D Tree Algorithm\n",
    "\n",
    "#### Brute Force Algorithm\n",
    "\n",
    "Brute Force Algorithms refers to a programming style that does not include any shortcuts to improve performance, but instead relies on sheer computing power to try all possibilities until the solution to a problem is found.\n",
    "\n",
    "#### K-D Tree Algorithm\n",
    "\n",
    "A k-d tree, or k-dimensional tree, is a data structure used in computer science for organizing some number of points in a space with k dimensions. It is a binary search tree with other constraints imposed on it. K-d trees are very useful for range and nearest neighbor searches. For our purposes we will generally only be dealing with point clouds in three dimensions, so all of our k-d trees will be three-dimensional. Each level of a k-d tree splits all children along a specific dimension, using a hyperplane that is perpendicular to the corresponding axis. At the root of the tree all children will be split based on the first dimension (i.e. if the first dimension coordinate is less than the root it will be in the left-sub tree and if it is greater than the root it will obviously be in the right sub-tree). \n",
    "\n",
    "Each level down in the tree divides on the next dimension, returning to the first dimension once all others have been exhausted. They most efficient way to build a k-d tree is to use a partition method like the one Quick Sort uses to place the median point at the root and everything with a smaller one dimensional value to the left and larger to the right. Then we repeat this procedure on both the left and right sub-trees until the last trees that we are to partition are only composed of one element.\n",
    "\n",
    "The basic idea is that if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant, without having to explicitly calculate their distance. (This may not be correct all the time.)\n",
    "\n",
    "#### Which one to choose when?\n",
    "\n",
    "The selection of the relevant algorithm for problem at hand depends on the number of dimensions and the size of training set.\n",
    "\n",
    "1. For small sample size and small dimensions, brute force performs well.\n",
    "2. Sparsity of data : If data is sparse with small dimensions (< 20) KD tree will perform better than Brute Force Algorithm.\n",
    "3. Value of K (neighbors) : As the K increases, query time of both KD tree and Brute Force Algorithm increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameter Tuning Techniques</h2>\n",
    "\n",
    "1. Grid Search\n",
    "\n",
    "2. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search for Hyperparameter tuning\n",
    "\n",
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n",
    "\n",
    "A model hyperparameter is a characteristic of a model that is external to the model and whose value cannot be estimated from data. The value of the hyperparameter has to be set before the learning process begins. For example, c in Support Vector Machines, k in k-Nearest Neighbors, the number of hidden layers in Neural Networks.\n",
    "\n",
    "Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Depending on the type of model utilized, certain parameters are necessary. Grid-searching can be applied across machine learning to calculate the best parameters to use for any given model. It is important to note that Grid-searching can be extremely computationally expensive and may take our machine quite a long time to run. Grid-Search will build a model on each parameter combination possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search for Hyperparameter tuning\n",
    "\n",
    "\n",
    "Random search (RS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized, and RS can hence be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.\n",
    "\n",
    "The name \"random search\" is attributed to Rastrigin[1] who made an early presentation of RS along with basic mathematical analysis. RS works by iteratively moving to better positions in the search-space, which are sampled from a hypersphere surrounding the current position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Performance Measurement of a Classification Model</h2>\n",
    "\n",
    "<h2>Confusion Matrix</h2>\n",
    "\n",
    "In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix.\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or “classifier”) on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n",
    "It allows easy identification of confusion between classes e.g. one class is commonly mislabeled as the other. Most performance measures are computed from the confusion matrix.\n",
    "\n",
    "A confusion matrix is a summary of prediction results on a classification problem.\n",
    "The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\n",
    "The confusion matrix shows the ways in which our classification model is confused when it makes predictions.\n",
    "It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of KNN\n",
    "\n",
    "1. The algorithm is simple and easy to implement.\n",
    "\n",
    "2. There’s no need to build a model, tune several parameters, or make additional assumptions.\n",
    "    \n",
    "3. This algorithm is mostly used for classification.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of KNN\n",
    "\n",
    "1. The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
    "\n",
    "2. KNN’s main disadvantage of becoming significantly slower as the volume of data increases makes it an impractical choice in environments where predictions need to be made rapidly. Moreover, there are faster algorithms that can produce more accurate classification and regression results.\n",
    "\n",
    "3. High memory requirement.\n",
    "\n",
    "4. Sensitive to outliers, accuracy is impacted by noise or irrelevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of KNN\n",
    "\n",
    "### Recommender Systems: \n",
    "\n",
    "1. At scale, this would look like recommending products on Amazon, articles on Medium, movies on Netflix, or videos on YouTube. Although, we can be certain they all use more efficient means of making recommendations due to the enormous volume of data they process.\n",
    "2. documents Searching : If I am searching for semantically similar documents (i.e., documents containing similar topics), this is referred to as Concept Search.\n",
    "3. movies on Netflix: The biggest use case of K-NN search might be Recommender Systems. If we are watching a comedy movie. Then netflix will Recommend me siimilar movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix's recommendation system\n",
    "\n",
    "We watch so many of the TV shows and Movies on Netflix. They are discovered through the platform’s recommendation system. That means the majority of what we decide to watch on Netflix is the result of decisions made by recommendation system.\n",
    "\n",
    "Netflix uses machine learning and algorithms to help break viewers’ preconceived notions and find shows that they might not have initially chosen. To do this, it looks at threads within the content, rather than relying on broad genres to make its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve classification problems. It’s easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
